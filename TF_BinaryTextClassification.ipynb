{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/sentiment labelled sentences/yelp_labelled.txt', names=['sentence','label'],sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  1000 non-null   object\n",
      " 1   label     1000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISeng cek jumlah total kata di dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10894"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cek = df['sentence'].str.split().apply(len).tolist()\n",
    "cek = pd.DataFrame(cek, columns=['words_count'])\n",
    "cek['words_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "kalimat = df['sentence'].values\n",
    "y = df['label'].values\n",
    "kalimat_train, kalimat_test, y_train, y_test = train_test_split(kalimat, y, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agar teks dapat dipahami oleh model, kita harus lakukan tokenisasi. Gunakan fungsi tokenizer pada data latih dan data test. Jangan lupa gunakan fungsi pad_sequences agar setiap sequence sama panjang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My wife had the Lobster Bisque soup which was lukewarm.'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kalimat_train[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ubah kalimat menjadi token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=750, oov_token='x')\n",
    "tokenizer.fit_on_texts(kalimat_train)\n",
    "tokenizer.fit_on_texts(kalimat_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mengubah token ke sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(kalimat_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(kalimat_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iseng cek jumlah maksimal kata dalam tiap row/sequences datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "maks = 0\n",
    "for n in train_sequences:\n",
    "    length = len(n)\n",
    "    if length>maks:\n",
    "        maks = length\n",
    "print(maks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 431, 23, 2, 432, 588, 351, 74, 5, 589]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "terlihat panjang tiap sequences berbeda, maka dikasih padding biar sama panjang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train = pad_sequences(train_sequences, maxlen=32)\n",
    "padded_test = pad_sequences(test_sequences, maxlen=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  21, 431,  23,   2,\n",
       "       432, 588, 351,  74,   5, 589], dtype=int32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "panjang tiap sequences sudah sama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      "32\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "for n in padded_train[1:5]:\n",
    "    length = len(n)\n",
    "    print(length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the Model with Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk arsitektur yang akan digunakan adalah layer embedding, dengan argumen pertama sesuai dengan jumlah vocabulary/kata yang kita pakai pada tokenizer. Argumen selanjutnya adalah dimensi embedding, dan input_length yang merupakan panjang dari sequence. Nah di kita tidak menggunakan layer Flatten melainkan kita menggantinya dengan GlobalAveragePooling1D. Fungsi ini bekerja lebih baik pada kasus NLP dibanding Flatten."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the vocabulary: The embedding dimension should be small enough to avoid overfitting, but large enough to capture the complexity of the vocabulary. As a rule of thumb, the dimensionality of the embedding vector should be proportional to the square root of the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(750, 27, input_length=32),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "25/25 - 2s - loss: 0.6930 - accuracy: 0.5475 - val_loss: 0.6920 - val_accuracy: 0.6650 - 2s/epoch - 65ms/step\n",
      "Epoch 2/30\n",
      "25/25 - 0s - loss: 0.6909 - accuracy: 0.6488 - val_loss: 0.6902 - val_accuracy: 0.7100 - 111ms/epoch - 4ms/step\n",
      "Epoch 3/30\n",
      "25/25 - 0s - loss: 0.6878 - accuracy: 0.7088 - val_loss: 0.6875 - val_accuracy: 0.6650 - 163ms/epoch - 7ms/step\n",
      "Epoch 4/30\n",
      "25/25 - 0s - loss: 0.6820 - accuracy: 0.7387 - val_loss: 0.6806 - val_accuracy: 0.7000 - 108ms/epoch - 4ms/step\n",
      "Epoch 5/30\n",
      "25/25 - 0s - loss: 0.6720 - accuracy: 0.7287 - val_loss: 0.6695 - val_accuracy: 0.6950 - 169ms/epoch - 7ms/step\n",
      "Epoch 6/30\n",
      "25/25 - 0s - loss: 0.6553 - accuracy: 0.7487 - val_loss: 0.6552 - val_accuracy: 0.7800 - 132ms/epoch - 5ms/step\n",
      "Epoch 7/30\n",
      "25/25 - 0s - loss: 0.6312 - accuracy: 0.7962 - val_loss: 0.6311 - val_accuracy: 0.7500 - 203ms/epoch - 8ms/step\n",
      "Epoch 8/30\n",
      "25/25 - 0s - loss: 0.5987 - accuracy: 0.8025 - val_loss: 0.6078 - val_accuracy: 0.8000 - 175ms/epoch - 7ms/step\n",
      "Epoch 9/30\n",
      "25/25 - 0s - loss: 0.5608 - accuracy: 0.8213 - val_loss: 0.5761 - val_accuracy: 0.8050 - 210ms/epoch - 8ms/step\n",
      "Epoch 10/30\n",
      "25/25 - 0s - loss: 0.5189 - accuracy: 0.8537 - val_loss: 0.5417 - val_accuracy: 0.8000 - 333ms/epoch - 13ms/step\n",
      "Epoch 11/30\n",
      "25/25 - 0s - loss: 0.4727 - accuracy: 0.8712 - val_loss: 0.5108 - val_accuracy: 0.8150 - 227ms/epoch - 9ms/step\n",
      "Epoch 12/30\n",
      "25/25 - 0s - loss: 0.4289 - accuracy: 0.8925 - val_loss: 0.4801 - val_accuracy: 0.8100 - 226ms/epoch - 9ms/step\n",
      "Epoch 13/30\n",
      "25/25 - 0s - loss: 0.3888 - accuracy: 0.9038 - val_loss: 0.4518 - val_accuracy: 0.8300 - 239ms/epoch - 10ms/step\n",
      "Epoch 14/30\n",
      "25/25 - 0s - loss: 0.3502 - accuracy: 0.9137 - val_loss: 0.4266 - val_accuracy: 0.8350 - 352ms/epoch - 14ms/step\n",
      "Epoch 15/30\n",
      "25/25 - 0s - loss: 0.3143 - accuracy: 0.9262 - val_loss: 0.4063 - val_accuracy: 0.8350 - 145ms/epoch - 6ms/step\n",
      "Epoch 16/30\n",
      "25/25 - 0s - loss: 0.2846 - accuracy: 0.9312 - val_loss: 0.3903 - val_accuracy: 0.8350 - 153ms/epoch - 6ms/step\n",
      "Epoch 17/30\n",
      "25/25 - 0s - loss: 0.2561 - accuracy: 0.9400 - val_loss: 0.3821 - val_accuracy: 0.8400 - 128ms/epoch - 5ms/step\n",
      "Epoch 18/30\n",
      "25/25 - 0s - loss: 0.2344 - accuracy: 0.9438 - val_loss: 0.3714 - val_accuracy: 0.8250 - 138ms/epoch - 6ms/step\n",
      "Epoch 19/30\n",
      "25/25 - 0s - loss: 0.2139 - accuracy: 0.9450 - val_loss: 0.3656 - val_accuracy: 0.8450 - 171ms/epoch - 7ms/step\n",
      "Epoch 20/30\n",
      "25/25 - 0s - loss: 0.1964 - accuracy: 0.9500 - val_loss: 0.3599 - val_accuracy: 0.8550 - 159ms/epoch - 6ms/step\n",
      "Epoch 21/30\n",
      "25/25 - 0s - loss: 0.1828 - accuracy: 0.9500 - val_loss: 0.3703 - val_accuracy: 0.8300 - 177ms/epoch - 7ms/step\n",
      "Epoch 22/30\n",
      "25/25 - 0s - loss: 0.1695 - accuracy: 0.9575 - val_loss: 0.3650 - val_accuracy: 0.8350 - 163ms/epoch - 7ms/step\n",
      "Epoch 23/30\n",
      "25/25 - 0s - loss: 0.1613 - accuracy: 0.9563 - val_loss: 0.3762 - val_accuracy: 0.8300 - 150ms/epoch - 6ms/step\n",
      "Epoch 24/30\n",
      "25/25 - 0s - loss: 0.1476 - accuracy: 0.9638 - val_loss: 0.3594 - val_accuracy: 0.8600 - 318ms/epoch - 13ms/step\n",
      "Epoch 25/30\n",
      "25/25 - 0s - loss: 0.1404 - accuracy: 0.9688 - val_loss: 0.3604 - val_accuracy: 0.8550 - 232ms/epoch - 9ms/step\n",
      "Epoch 26/30\n",
      "25/25 - 0s - loss: 0.1322 - accuracy: 0.9638 - val_loss: 0.3654 - val_accuracy: 0.8550 - 152ms/epoch - 6ms/step\n",
      "Epoch 27/30\n",
      "25/25 - 0s - loss: 0.1229 - accuracy: 0.9712 - val_loss: 0.3760 - val_accuracy: 0.8500 - 139ms/epoch - 6ms/step\n",
      "Epoch 28/30\n",
      "25/25 - 0s - loss: 0.1160 - accuracy: 0.9725 - val_loss: 0.3736 - val_accuracy: 0.8550 - 273ms/epoch - 11ms/step\n",
      "Epoch 29/30\n",
      "25/25 - 0s - loss: 0.1092 - accuracy: 0.9762 - val_loss: 0.3757 - val_accuracy: 0.8550 - 252ms/epoch - 10ms/step\n",
      "Epoch 30/30\n",
      "25/25 - 0s - loss: 0.1053 - accuracy: 0.9775 - val_loss: 0.3801 - val_accuracy: 0.8550 - 277ms/epoch - 11ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(padded_train, y_train, epochs=30,\n",
    "                    validation_data=(padded_test, y_test), \n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9346712bfa21619f0b33b5368f9efedb0d5277a757a7f667797f38d524b1782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
