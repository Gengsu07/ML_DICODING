{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNL7g0Cp99M67Ikl/R+KRkb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gengsu07/ML_DICODING/blob/main/NLP_TOKENIZATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Melakukan tokenization dan membuat sequence dari teks"
      ],
      "metadata": {
        "id": "BFR9RVtNllwl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6ZvZVVxxlNN2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "num_words: This parameter specifies the maximum number of words to keep based on word frequency. Only the most common words in the corpus will be kept, and less frequent words will be discarded. This can help reduce the size of the vocabulary and improve training performance.\n",
        "\n",
        "oov_token: This parameter specifies the token to use for out-of-vocabulary words (i.e., words that are not present in the word index). When oov_token is specified, any word that is not in the word index will be replaced by this token.\n",
        "\n",
        "lower: This parameter specifies whether to convert all text to lowercase. This can help reduce the size of the vocabulary and make the model more robust to variations in capitalization.\n",
        "\n",
        "char_level: This parameter specifies whether to tokenize the text at the character level instead of the word level. When char_level=True, each character in the input text will be treated as a separate token.\n",
        "\n",
        "filters: This parameter specifies a string containing characters that should be filtered out from the input text. For example, if filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_{|}~\\t\\n'`, any of these characters that appear in the input text will be removed.\n",
        "\n",
        "split: This parameter specifies the string to use for splitting the input text into tokens. The default value is ' ', which splits the text at whitespace characters. You can also specify other delimiters such as ',' or ';'."
      ],
      "metadata": {
        "id": "dgJFCpcpm370"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=15, oov_token='-')"
      ],
      "metadata": {
        "id": "u5113T7DluPZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teks = ['saya adalah seorang pemberani',\n",
        "        'suka belajar teknologi dan hal baru',\n",
        "        'berusaha selalu memperbaiki diri',\n",
        "        'bersiap menuju hari akhir ']"
      ],
      "metadata": {
        "id": "-NQi9qeQnFL8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk melakukan tokenisasi, panggil fungsi fit_on_text() pada objek tokenizer dan isi teks kita sebagai argumennya."
      ],
      "metadata": {
        "id": "YImjGckwndYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(teks)"
      ],
      "metadata": {
        "id": "2w1hNaCmnbUs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kemudian, kita akan mengubah text yang telah dibuat sebelumnya ke dalam bentuk sequence menggunakan fungsi text_to_sequences."
      ],
      "metadata": {
        "id": "tftyMDvjnuF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(teks)"
      ],
      "metadata": {
        "id": "aIs3d2xPnitB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk melihat hasil tokenisasi, kita bisa memanggil atribut word_index dari objek tokenizer. Atribut word index mengembalikan dictionary berupa kata sebagai key dan token atau nilai numeriknya sebagai value. Perlu diperhatikan bahwa tanda baca dan huruf kapital tidak diproses oleh tokenizer. Contohnya kata “Selamat!” dan “SELAMAT” akan diperlakukan sebagai kata yang sama. Hasil dari cell di bawah menunjukkan bahwa kata-kata yang out-of-vocabulary akan diberi token bernilai 1. "
      ],
      "metadata": {
        "id": "Q8hH7y4MoG2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LcNt2bwVn7Di",
        "outputId": "ccfd11ba-3569-4edd-cb33-2c9167ad7036"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'-': 1, 'saya': 2, 'adalah': 3, 'seorang': 4, 'pemberani': 5, 'suka': 6, 'belajar': 7, 'teknologi': 8, 'dan': 9, 'hal': 10, 'baru': 11, 'berusaha': 12, 'selalu': 13, 'memperbaiki': 14, 'diri': 15, 'bersiap': 16, 'menuju': 17, 'hari': 18, 'akhir': 19}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ". Output kode di bawah merupakan contoh penggunaan token untuk mengubah kalimat ke dalam bentuk numerik. Pada contoh tersebut, kata ‘belajar’, ‘sejak’, dan ‘SMP’ ditandai dengan nilai \"1\". Hal ini menunjukkan bahwa kata-kata tersebut tidak terdapat pada dictionary yang sebelumnya telah dibuat (OOV). Tanpa OOV, kata yang tidak memiliki token tidak dimasukkan pada sequence. Jika kita menggunakan OOV, maka setiap kata yang tidak memiliki token akan diberikan token yang seragam. Dengan OOV, informasi urutan setiap kata pada kalimat tidak hilang."
      ],
      "metadata": {
        "id": "dUSOt5EypcuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.texts_to_sequences(['suka belajar teknologi dan hal baru']))\n",
        "print(tokenizer.texts_to_sequences(['berusaha selalu memperbaiki diri']))\n",
        "print(tokenizer.texts_to_sequences(['bersiap menuju hari akhir ']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rzubkJ2ZoJDX",
        "outputId": "83e1dada-e1fc-4954-9906-15d73cfd650b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[6, 7, 8, 9, 10, 11]]\n",
            "[[12, 13, 14, 1]]\n",
            "[[1, 1, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Terlihat bahwa yg =1 tidak ter tokenize karena num_words cuman=15, jadi tetap tertangkap kata tersebut dgn token yg sama daripada informasinya dihilangkan"
      ],
      "metadata": {
        "id": "Z3qnJEvwp2aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "q2BuyH9WpqXF",
        "outputId": "a67fd992-f962-4997-80a2-6f04360c944e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 3, 4, 5], [6, 7, 8, 9, 10, 11], [12, 13, 14, 1], [1, 1, 1, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Ketika sequence telah dibuat, hal yang perlu kita lakukan adalah padding. Yup, padding adalah proses untuk membuat setiap kalimat pada teks memiliki panjang yang seragam. Sama seperti melakukan resize gambar, agar resolusi setiap gambar sama besar. "
      ],
      "metadata": {
        "id": "cFcrGhlWqVL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "sequences_sama_panjang = pad_sequences(sequences)"
      ],
      "metadata": {
        "id": "DtAK4f5nqNO8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_sama_panjang"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cX4W_Mi8qjU6",
        "outputId": "9f8f88f6-f59f-44e9-c33d-c7b10c47f879"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  2,  3,  4,  5],\n",
              "       [ 6,  7,  8,  9, 10, 11],\n",
              "       [ 0,  0, 12, 13, 14,  1],\n",
              "       [ 0,  0,  1,  1,  1,  1]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "setiap sequence akan memiliki panjang yang sama. Padding dapat melakukan ini dengan menambahkan 0 secara default pada awal sequence yang lebih pendek\n",
        "\n",
        "Jika kita ingin merubah sehingga 0 ditambahkan di akhir sequence, kita dapat menggunakan parameter padding dengan nilai ‘post’. Selain itu kita dapat mengatur parameter maxlen (panjang maksimum setiap sequence) dengan nilai yang kita inginkan. Jika kita mengisi nilai 5, maka panjang sebuah sequence tidak akan melebihi 5. \n",
        "\n"
      ],
      "metadata": {
        "id": "_H6PNqwgqtHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_sama_panjang = pad_sequences(sequences,\n",
        "                                       padding='post',\n",
        "                                       maxlen = 5)"
      ],
      "metadata": {
        "id": "NThmzNICqlpu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_sama_panjang"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eUtJU9norIAD",
        "outputId": "74a0a45c-a041-4910-b142-95cea252c8d5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2,  3,  4,  5,  0],\n",
              "       [ 7,  8,  9, 10, 11],\n",
              "       [12, 13, 14,  1,  0],\n",
              "       [ 1,  1,  1,  1,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jika teks kita memiliki panjang lebih dari nilai parameter maxlen misalnya 5, maka secara default nilai dari sequence akan diambil 5 nilai terakhir atau 5 kata terakhir saja dari setiap kalimat (mengabaikan kata sebelumnya). \n",
        "\n",
        "Untuk mengubah pengaturan ini dan mengambil 5 kata awal dari tiap kalimat, kita dapat menggunakan parameter truncating dan mengisi nilai ‘post’."
      ],
      "metadata": {
        "id": "hkPn6ByvrXGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_sama_panjang = pad_sequences(sequences,\n",
        "                                       padding='post',\n",
        "                                       maxlen = 5,\n",
        "                                       truncating = 'post')"
      ],
      "metadata": {
        "id": "CZtjXra1rJ_S"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_sama_panjang"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NJT5hMmirmX0",
        "outputId": "3389b19f-8312-4814-f857-de3ebdec3194"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2,  3,  4,  5,  0],\n",
              "       [ 6,  7,  8,  9, 10],\n",
              "       [12, 13, 14,  1,  0],\n",
              "       [ 1,  1,  1,  1,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QN0znvY6roHe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}